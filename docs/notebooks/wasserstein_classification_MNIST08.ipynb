{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: HKR classifier on MNIST dataset\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deel-ai/deel-torchlip/blob/master/docs/notebooks/wasserstein_classification_MNIST08.ipynb)\n",
    "\n",
    "This notebook demonstrates how to learn a binary classifier on the MNIST0-8 dataset (MNIST with only 0 and 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required library deel-torchlip (uncomment line below)\n",
    "# %pip install -qqq deel-torchlip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "For this task we will select two classes: 0 and 8. Labels are changed to {-1,1}, which\n",
    "is compatible with the hinge term used in the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 11774 samples, classes proportions: 50.31 %\n",
      "Test set size: 1954 samples, classes proportions: 50.15 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "\n",
    "# First we select the two classes\n",
    "selected_classes = [0, 8]  # must be two classes as we perform binary classification\n",
    "\n",
    "\n",
    "def prepare_data(dataset, class_a=0, class_b=8):\n",
    "    \"\"\"\n",
    "    This function converts the MNIST data to make it suitable for our binary\n",
    "    classification setup.\n",
    "    \"\"\"\n",
    "    x = dataset.data\n",
    "    y = dataset.targets\n",
    "    # select items from the two selected classes\n",
    "    mask = (y == class_a) + (\n",
    "        y == class_b\n",
    "    )  # mask to select only items from class_a or class_b\n",
    "    x = x[mask]\n",
    "    y = y[mask]\n",
    "\n",
    "    # convert from range int[0,255] to float32[-1,1]\n",
    "    x = x.float() / 255\n",
    "    x = x.reshape((-1, 28, 28, 1))\n",
    "    # change label to binary classification {-1,1}\n",
    "\n",
    "    y_ = torch.zeros_like(y).float()\n",
    "    y_[y == class_a] = 1.0\n",
    "    y_[y == class_b] = -1.0\n",
    "    return torch.utils.data.TensorDataset(x, y_)\n",
    "\n",
    "\n",
    "train = datasets.MNIST(\"./data\", train=True, download=True)\n",
    "test = datasets.MNIST(\"./data\", train=False, download=True)\n",
    "\n",
    "# Prepare the data\n",
    "train = prepare_data(train, selected_classes[0], selected_classes[1])\n",
    "test = prepare_data(test, selected_classes[0], selected_classes[1])\n",
    "\n",
    "# Display infos about dataset\n",
    "print(\n",
    "    f\"Train set size: {len(train)} samples, classes proportions: \"\n",
    "    f\"{100 * (train.tensors[1] == 1).numpy().mean():.2f} %\"\n",
    ")\n",
    "print(\n",
    "    f\"Test set size: {len(test)} samples, classes proportions: \"\n",
    "    f\"{100 * (test.tensors[1] == 1).numpy().mean():.2f} %\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Lipschitz model\n",
    "\n",
    "Here, the experiments are done with a model with only fully-connected layers. However,\n",
    "`torchlip` also provides state-of-the-art 1-Lipschitz convolutional layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequential model contains a layer which is not a Lipschitz layer: Flatten(start_dim=1, end_dim=-1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): SpectralLinear(in_features=784, out_features=128, bias=True)\n",
       "  (2): FullSort()\n",
       "  (3): SpectralLinear(in_features=128, out_features=64, bias=True)\n",
       "  (4): FullSort()\n",
       "  (5): SpectralLinear(in_features=64, out_features=32, bias=True)\n",
       "  (6): FullSort()\n",
       "  (7): FrobeniusLinear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from deel import torchlip\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ninputs = 28 * 28\n",
    "wass = torchlip.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torchlip.SpectralLinear(ninputs, 128),\n",
    "    torchlip.FullSort(),\n",
    "    torchlip.SpectralLinear(128, 64),\n",
    "    torchlip.FullSort(),\n",
    "    torchlip.SpectralLinear(64, 32),\n",
    "    torchlip.FullSort(),\n",
    "    torchlip.FrobeniusLinear(32, 1),\n",
    ").to(device)\n",
    "\n",
    "wass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learn classification on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "loss: -2.7467 - KR: 1.4650 - acc: 0.7802 - val_loss: -2.5997 - val_KR: 2.9414 - val_acc: 0.9928\n",
      "Epoch 2/10\n",
      "loss: -3.9833 - KR: 3.8255 - acc: 0.9909 - val_loss: -4.4334 - val_KR: 4.6879 - val_acc: 0.9918\n",
      "Epoch 3/10\n",
      "loss: -5.4598 - KR: 5.1827 - acc: 0.9911 - val_loss: -5.4592 - val_KR: 5.7215 - val_acc: 0.9903\n",
      "Epoch 4/10\n",
      "loss: -5.9852 - KR: 5.9702 - acc: 0.9907 - val_loss: -6.0463 - val_KR: 6.2968 - val_acc: 0.9903\n",
      "Epoch 5/10\n",
      "loss: -6.1642 - KR: 6.3862 - acc: 0.9918 - val_loss: -6.3093 - val_KR: 6.5812 - val_acc: 0.9882\n",
      "Epoch 6/10\n",
      "loss: -6.0871 - KR: 6.6226 - acc: 0.9921 - val_loss: -6.4519 - val_KR: 6.7566 - val_acc: 0.9877\n",
      "Epoch 7/10\n",
      "loss: -6.6933 - KR: 6.7737 - acc: 0.9920 - val_loss: -6.6401 - val_KR: 6.8724 - val_acc: 0.9918\n",
      "Epoch 8/10\n",
      "loss: -6.9351 - KR: 6.8675 - acc: 0.9932 - val_loss: -6.7282 - val_KR: 6.9290 - val_acc: 0.9923\n",
      "Epoch 9/10\n",
      "loss: -6.6895 - KR: 6.9427 - acc: 0.9935 - val_loss: -6.7878 - val_KR: 7.0178 - val_acc: 0.9913\n",
      "Epoch 10/10\n",
      "loss: -7.1714 - KR: 6.9925 - acc: 0.9931 - val_loss: -6.8234 - val_KR: 7.0566 - val_acc: 0.9908\n"
     ]
    }
   ],
   "source": [
    "from deel.torchlip.functional import kr_loss, hkr_loss, hinge_margin_loss\n",
    "\n",
    "# training parameters\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# loss parameters\n",
    "min_margin = 1\n",
    "alpha = 10\n",
    "\n",
    "optimizer = torch.optim.Adam(lr=0.001, params=wass.parameters())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    m_kr, m_hm, m_acc = 0, 0, 0\n",
    "    wass.train()\n",
    "\n",
    "    for step, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = wass(data)\n",
    "        loss = hkr_loss(output, target, alpha=alpha, min_margin=min_margin)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute metrics on batch\n",
    "        m_kr += kr_loss(output, target, (1, -1))\n",
    "        m_hm += hinge_margin_loss(output, target, min_margin)\n",
    "        m_acc += (torch.sign(output).flatten() == torch.sign(target)).sum() / len(\n",
    "            target\n",
    "        )\n",
    "\n",
    "    # Train metrics for the current epoch\n",
    "    metrics = [\n",
    "        f\"{k}: {v:.04f}\"\n",
    "        for k, v in {\n",
    "            \"loss\": loss,\n",
    "            \"KR\": m_kr / (step + 1),\n",
    "            \"acc\": m_acc / (step + 1),\n",
    "        }.items()\n",
    "    ]\n",
    "\n",
    "    # Compute test loss for the current epoch\n",
    "    wass.eval()\n",
    "    testo = []\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        testo.append(wass(data).detach().cpu())\n",
    "    testo = torch.cat(testo).flatten()\n",
    "\n",
    "    # Validation metrics for the current epoch\n",
    "    metrics += [\n",
    "        f\"val_{k}: {v:.04f}\"\n",
    "        for k, v in {\n",
    "            \"loss\": hkr_loss(\n",
    "                testo, test.tensors[1], alpha=alpha, min_margin=min_margin\n",
    "            ),\n",
    "            \"KR\": kr_loss(testo.flatten(), test.tensors[1], (1, -1)),\n",
    "            \"acc\": (torch.sign(testo).flatten() == torch.sign(test.tensors[1]))\n",
    "            .float()\n",
    "            .mean(),\n",
    "        }.items()\n",
    "    ]\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(\" - \".join(metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Lipschitz constant of our networks\n",
    "\n",
    "### 4.1. Empirical evaluation\n",
    "\n",
    "We can estimate the Lipschitz constant by evaluating \n",
    "\n",
    "$$\n",
    "    \\frac{\\Vert{}F(x_2) - F(x_1)\\Vert{}}{\\Vert{}x_2 - x_1\\Vert{}} \\quad\\text{or}\\quad \n",
    "    \\frac{\\Vert{}F(x + \\epsilon) - F(x)\\Vert{}}{\\Vert{}\\epsilon\\Vert{}}\n",
    "$$\n",
    "\n",
    "for various inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1304)\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "wass.eval()\n",
    "\n",
    "p = []\n",
    "for _ in range(64):\n",
    "    eps = 1e-3\n",
    "    batch, _ = next(iter(train_loader))\n",
    "    dist = torch.distributions.Uniform(-eps, +eps).sample(batch.shape)\n",
    "    y1 = wass(batch.to(device)).detach().cpu()\n",
    "    y2 = wass((batch + dist).to(device)).detach().cpu()\n",
    "\n",
    "    p.append(\n",
    "        torch.max(\n",
    "            torch.norm(y2 - y1, dim=1)\n",
    "            / torch.norm(dist.reshape(dist.shape[0], -1), dim=1)\n",
    "        )\n",
    "    )\n",
    "print(torch.tensor(p).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9169, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "p = []\n",
    "for batch, _ in train_loader:\n",
    "    x = batch.numpy()\n",
    "    y = wass(batch.to(device)).detach().cpu().numpy()\n",
    "    xd = pdist(x.reshape(batch.shape[0], -1))\n",
    "    yd = pdist(y.reshape(batch.shape[0], -1))\n",
    "\n",
    "    p.append((yd / xd).max())\n",
    "print(torch.tensor(p).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, using the $\\epsilon$-version, we greatly under-estimate the Lipschitz constant.\n",
    "Using the train dataset, we find a Lipschitz constant close to 0.9, which is better, but our network should be 1-Lipschitz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Singular-Value Decomposition\n",
    "\n",
    "Since our network is only made of linear layers and `FullSort` activation, we can compute *Singular-Value Decomposition* (SVD) of our weight matrix and check that, for each linear layer, all singular values are 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Before export ===\n",
      "SpectralLinear(in_features=784, out_features=128, bias=True), min=0.9999998807907104, max=1.0\n",
      "SpectralLinear(in_features=128, out_features=64, bias=True), min=0.9999999403953552, max=1.0000001192092896\n",
      "SpectralLinear(in_features=64, out_features=32, bias=True), min=0.9999998211860657, max=1.000000238418579\n",
      "FrobeniusLinear(in_features=32, out_features=1, bias=True), min=1.0, max=1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Before export ===\")\n",
    "layers = list(wass.children())\n",
    "for layer in layers:\n",
    "    if hasattr(layer, \"weight\"):\n",
    "        w = layer.weight\n",
    "        u, s, v = torch.svd(w)\n",
    "        print(f\"{layer}, min={s.min()}, max={s.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== After export ===\n",
      "Linear(in_features=784, out_features=128, bias=True), min=0.9999998807907104, max=1.0\n",
      "Linear(in_features=128, out_features=64, bias=True), min=0.9999999403953552, max=1.0000001192092896\n",
      "Linear(in_features=64, out_features=32, bias=True), min=0.9999998211860657, max=1.000000238418579\n",
      "Linear(in_features=32, out_features=1, bias=True), min=1.0, max=1.0\n"
     ]
    }
   ],
   "source": [
    "wexport = wass.vanilla_export()\n",
    "\n",
    "print(\"=== After export ===\")\n",
    "layers = list(wexport.children())\n",
    "for layer in layers:\n",
    "    if hasattr(layer, \"weight\"):\n",
    "        w = layer.weight\n",
    "        u, s, v = torch.svd(w)\n",
    "        print(f\"{layer}, min={s.min()}, max={s.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all our singular values are very close to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
