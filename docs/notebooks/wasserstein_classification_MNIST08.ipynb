{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: HKR classifier on MNIST dataset\n",
    "\n",
    "This notebook demonstrates how to learn a binary classifier on the MNIST0-8 dataset (MNIST with only 0 and 8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "For this task we will select two classes: 0 and 8. Labels are changed to {-1,1}, which\n",
    "is compatible with the hinge term used in the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 11774 samples, classes proportions: 50.31 %\n",
      "Test set size: 1954 samples, classes proportions: 50.15 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "\n",
    "# First we select the two classes\n",
    "selected_classes = [0, 8]  # must be two classes as we perform binary classification\n",
    "\n",
    "\n",
    "def prepare_data(dataset, class_a=0, class_b=8):\n",
    "    \"\"\"\n",
    "    This function converts the MNIST data to make it suitable for our binary\n",
    "    classification setup.\n",
    "    \"\"\"\n",
    "    x = dataset.data\n",
    "    y = dataset.targets\n",
    "    # select items from the two selected classes\n",
    "    mask = (y == class_a) + (\n",
    "        y == class_b\n",
    "    )  # mask to select only items from class_a or class_b\n",
    "    x = x[mask]\n",
    "    y = y[mask]\n",
    "\n",
    "    # convert from range int[0,255] to float32[-1,1]\n",
    "    x = x.float() / 255\n",
    "    x = x.reshape((-1, 28, 28, 1))\n",
    "    # change label to binary classification {-1,1}\n",
    "\n",
    "    y_ = torch.zeros_like(y).float()\n",
    "    y_[y == class_a] = 1.0\n",
    "    y_[y == class_b] = -1.0\n",
    "    return torch.utils.data.TensorDataset(x, y_)\n",
    "\n",
    "\n",
    "train = datasets.MNIST(\"./data\", train=True, download=True)\n",
    "test = datasets.MNIST(\"./data\", train=False, download=True)\n",
    "\n",
    "# Prepare the data\n",
    "train = prepare_data(train, selected_classes[0], selected_classes[1])\n",
    "test = prepare_data(test, selected_classes[0], selected_classes[1])\n",
    "\n",
    "# Display infos about dataset\n",
    "print(\n",
    "    f\"Train set size: {len(train)} samples, classes proportions: \"\n",
    "    f\"{100 * (train.tensors[1] == 1).numpy().mean():.2f} %\"\n",
    ")\n",
    "print(\n",
    "    f\"Test set size: {len(test)} samples, classes proportions: \"\n",
    "    f\"{100 * (test.tensors[1] == 1).numpy().mean():.2f} %\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Lipschitz model\n",
    "\n",
    "Here, the experiments are done with a model with only fully-connected layers. However,\n",
    "`torchlip` also provides state-of-the-art 1-Lipschitz convolutional layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequential model contains a layer which is not a Lipschitz layer: Flatten(start_dim=1, end_dim=-1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): SpectralLinear(in_features=784, out_features=128, bias=True)\n",
       "  (2): FullSort()\n",
       "  (3): SpectralLinear(in_features=128, out_features=64, bias=True)\n",
       "  (4): FullSort()\n",
       "  (5): SpectralLinear(in_features=64, out_features=32, bias=True)\n",
       "  (6): FullSort()\n",
       "  (7): FrobeniusLinear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from deel import torchlip\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ninputs = 28 * 28\n",
    "wass = torchlip.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torchlip.SpectralLinear(ninputs, 128),\n",
    "    torchlip.FullSort(),\n",
    "    torchlip.SpectralLinear(128, 64),\n",
    "    torchlip.FullSort(),\n",
    "    torchlip.SpectralLinear(64, 32),\n",
    "    torchlip.FullSort(),\n",
    "    torchlip.FrobeniusLinear(32, 1),\n",
    ").to(device)\n",
    "\n",
    "wass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learn classification on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:01<00:00, 60.63it/s, loss=-3.0010, kr=1.9975, hinge=0.2509, acc=0.9332, val_loss=-3.0327, val_kr=3.2924, val_hinge=0.0260, val_acc=0.9928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:01<00:00, 60.22it/s, loss=-4.4525, kr=4.1030, hinge=0.0254, acc=0.9938, val_loss=-4.8342, val_kr=5.0409, val_hinge=0.0207, val_acc=0.9923]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:01<00:00, 60.08it/s, loss=-5.6243, kr=5.6371, hinge=0.0241, acc=0.9926, val_loss=-5.9461, val_kr=6.1530, val_hinge=0.0207, val_acc=0.9918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:01<00:00, 60.13it/s, loss=-6.3080, kr=6.3542, hinge=0.0227, acc=0.9923, val_loss=-6.3689, val_kr=6.5977, val_hinge=0.0229, val_acc=0.9918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:01<00:00, 60.48it/s, loss=-6.4896, kr=6.6610, hinge=0.0216, acc=0.9930, val_loss=-6.5609, val_kr=6.7654, val_hinge=0.0204, val_acc=0.9923]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:01<00:00, 60.28it/s, loss=-6.5060, kr=6.8111, hinge=0.0202, acc=0.9930, val_loss=-6.6961, val_kr=6.9176, val_hinge=0.0221, val_acc=0.9918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:01<00:00, 60.94it/s, loss=-6.7015, kr=6.9254, hinge=0.0202, acc=0.9935, val_loss=-6.7808, val_kr=6.9808, val_hinge=0.0200, val_acc=0.9928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:01<00:00, 60.25it/s, loss=-6.7822, kr=6.9913, hinge=0.0187, acc=0.9935, val_loss=-6.8321, val_kr=7.0534, val_hinge=0.0221, val_acc=0.9923]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:01<00:00, 59.73it/s, loss=-6.6593, kr=7.0458, hinge=0.0190, acc=0.9937, val_loss=-6.8656, val_kr=7.0789, val_hinge=0.0213, val_acc=0.9928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:01<00:00, 60.08it/s, loss=-7.0013, kr=7.0847, hinge=0.0182, acc=0.9941, val_loss=-6.9047, val_kr=7.1115, val_hinge=0.0207, val_acc=0.9928]\n"
     ]
    }
   ],
   "source": [
    "from deel.torchlip.functional import kr_loss, hkr_loss, hinge_margin_loss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# training parameters\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# loss parameters\n",
    "min_margin = 1\n",
    "alpha = 10\n",
    "\n",
    "optimizer = torch.optim.Adam(lr=0.001, params=wass.parameters())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    m_kr, m_hm, m_acc = 0, 0, 0\n",
    "    wass.train()\n",
    "\n",
    "    with tqdm(total=len(train_loader)) as tsteps:\n",
    "        for step, (data, target) in enumerate(train_loader):\n",
    "            tsteps.update()\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = wass(data)\n",
    "            loss = hkr_loss(output, target, alpha=alpha, min_margin=min_margin)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute metrics on batch\n",
    "            m_kr += kr_loss(output, target, (1, -1))\n",
    "            m_hm += hinge_margin_loss(output, target, min_margin)\n",
    "            m_acc += (torch.sign(output).flatten() == torch.sign(target)).sum() / len(\n",
    "                target\n",
    "            )\n",
    "\n",
    "            # Print metrics of current batch\n",
    "            postfix = {\n",
    "                k: \"{:.04f}\".format(v)\n",
    "                for k, v in {\n",
    "                    \"loss\": loss,\n",
    "                    \"kr\": m_kr / (step + 1),\n",
    "                    \"hinge\": m_hm / (step + 1),\n",
    "                    \"acc\": m_acc / (step + 1),\n",
    "                }.items()\n",
    "            }\n",
    "            tsteps.set_postfix(postfix)\n",
    "\n",
    "        # Compute test loss for the current epoch\n",
    "        wass.eval()\n",
    "        testo = []\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            testo.append(wass(data).detach().cpu())\n",
    "        testo = torch.cat(testo).flatten()\n",
    "\n",
    "        # Print metrics for the current epoch (train and validation metrics)\n",
    "        postfix.update(\n",
    "            {\n",
    "                f\"val_{k}\": \"{:.04f}\".format(v)\n",
    "                for k, v in {\n",
    "                    \"loss\": hkr_loss(\n",
    "                        testo, test.tensors[1], alpha=alpha, min_margin=min_margin\n",
    "                    ),\n",
    "                    \"kr\": kr_loss(testo.flatten(), test.tensors[1], (1, -1)),\n",
    "                    \"hinge\": hinge_margin_loss(\n",
    "                        testo.flatten(), test.tensors[1], min_margin\n",
    "                    ),\n",
    "                    \"acc\": (torch.sign(testo).flatten() == torch.sign(test.tensors[1]))\n",
    "                    .float()\n",
    "                    .mean(),\n",
    "                }.items()\n",
    "            }\n",
    "        )\n",
    "        tsteps.set_postfix(postfix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Lipschitz constant of our networks\n",
    "\n",
    "### 4.1. Empirical evaluation\n",
    "\n",
    "We can estimate the Lipschitz constant by evaluating \n",
    "\n",
    "$$\n",
    "    \\frac{\\Vert{}F(x_2) - F(x_1)\\Vert{}}{\\Vert{}x_2 - x_1\\Vert{}} \\quad\\text{or}\\quad \n",
    "    \\frac{\\Vert{}F(x + \\epsilon) - F(x)\\Vert{}}{\\Vert{}\\epsilon\\Vert{}}\n",
    "$$\n",
    "\n",
    "for various inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1517)\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "wass.eval()\n",
    "\n",
    "p = []\n",
    "for _ in range(64):\n",
    "    eps = 1e-3\n",
    "    batch, _ = next(iter(train_loader))\n",
    "    dist = torch.distributions.Uniform(-eps, +eps).sample(batch.shape)\n",
    "    y1 = wass(batch.to(device)).detach().cpu()\n",
    "    y2 = wass((batch + dist).to(device)).detach().cpu()\n",
    "\n",
    "    p.append(\n",
    "        torch.max(\n",
    "            torch.norm(y2 - y1, dim=1)\n",
    "            / torch.norm(dist.reshape(dist.shape[0], -1), dim=1)\n",
    "        )\n",
    "    )\n",
    "print(torch.tensor(p).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 168.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9063, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "wass.eval()\n",
    "\n",
    "p = []\n",
    "for batch, _ in tqdm(train_loader):\n",
    "    x = batch.numpy()\n",
    "    y = wass(batch.to(device)).detach().cpu().numpy()\n",
    "    xd = pdist(x.reshape(batch.shape[0], -1))\n",
    "    yd = pdist(y.reshape(batch.shape[0], -1))\n",
    "\n",
    "    p.append((yd / xd).max())\n",
    "print(torch.tensor(p).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, using the $\\epsilon$-version, we greatly under-estimate the Lipschitz constant.\n",
    "Using the train dataset, we find a Lipschitz constant close to 0.9, which is better, but our network should be 1-Lipschitz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Singular-Value Decomposition\n",
    "\n",
    "Since our network is only made of linear layers and `FullSort` activation, we can compute *Singular-Value Decomposition* (SVD) of our weight matrix and check that, for each linear layer, all singular values are 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Before export ===\n",
      "SpectralLinear(in_features=784, out_features=128, bias=True), min=0.9999998807907104, max=1.0\n",
      "SpectralLinear(in_features=128, out_features=64, bias=True), min=0.9999998807907104, max=1.0000001192092896\n",
      "SpectralLinear(in_features=64, out_features=32, bias=True), min=0.9999998807907104, max=1.0\n",
      "FrobeniusLinear(in_features=32, out_features=1, bias=True), min=0.9999999403953552, max=0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Before export ===\")\n",
    "layers = list(wass.children())\n",
    "for layer in layers:\n",
    "    if hasattr(layer, \"weight\"):\n",
    "        w = layer.weight\n",
    "        u, s, v = torch.svd(w)\n",
    "        print(f\"{layer}, min={s.min()}, max={s.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== After export ===\n",
      "Linear(in_features=784, out_features=128, bias=True), min=0.9999998807907104, max=1.0\n",
      "Linear(in_features=128, out_features=64, bias=True), min=0.9999998807907104, max=1.0000001192092896\n",
      "Linear(in_features=64, out_features=32, bias=True), min=0.9999998807907104, max=1.0\n",
      "Linear(in_features=32, out_features=1, bias=True), min=0.9999999403953552, max=0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "wexport = wass.vanilla_export()\n",
    "\n",
    "print(\"=== After export ===\")\n",
    "layers = list(wexport.children())\n",
    "for layer in layers:\n",
    "    if hasattr(layer, \"weight\"):\n",
    "        w = layer.weight\n",
    "        u, s, v = torch.svd(w)\n",
    "        print(f\"{layer}, min={s.min()}, max={s.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all our singular values are very close to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
