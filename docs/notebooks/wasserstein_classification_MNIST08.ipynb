{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: HKR classifier on MNIST dataset\n",
    "\n",
    "This notebook demonstrates how to learn a binary classifier on the MNIST0-8 dataset (MNIST with only 0 and 8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "For this task we will select two classes: 0 and 8. Labels are changed to {-1,1}, wich is compatible\n",
    "with the Hinge term used in the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 11774 samples, classes proportions: 50.306 percent\n",
      "test set size: 1954 samples, classes proportions: 50.154 percent\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision import datasets\n",
    "\n",
    "# first we select the two classes\n",
    "selected_classes = [0, 8]  # must be two classes as we perform binary classification\n",
    "\n",
    "\n",
    "def prepare_data(dataset, class_a=0, class_b=8):\n",
    "    \"\"\"\n",
    "    This function convert the MNIST data to make it suitable for our binary classification\n",
    "    setup.\n",
    "    \"\"\"\n",
    "    x = dataset.data\n",
    "    y = dataset.targets\n",
    "    # select items from the two selected classes\n",
    "    mask = (y == class_a) + (\n",
    "        y == class_b\n",
    "    )  # mask to select only items from class_a or class_b\n",
    "    x = x[mask]\n",
    "    y = y[mask]\n",
    "\n",
    "    # convert from range int[0,255] to float32[-1,1]\n",
    "    x = x.float() / 255\n",
    "    x = x.reshape((-1, 28, 28, 1))\n",
    "    # change label to binary classification {-1,1}\n",
    "\n",
    "    y_ = torch.zeros_like(y).float()\n",
    "    y_[y == class_a] = 1.0\n",
    "    y_[y == class_b] = -1.0\n",
    "    return torch.utils.data.TensorDataset(x, y_)\n",
    "\n",
    "\n",
    "train = datasets.MNIST(\"./data\", train=True, download=True)\n",
    "test = datasets.MNIST(\"./data\", train=False, download=True)\n",
    "\n",
    "# prepare the data\n",
    "train = prepare_data(train, selected_classes[0], selected_classes[1])\n",
    "test = prepare_data(test, selected_classes[0], selected_classes[1])\n",
    "\n",
    "# display infos about dataset\n",
    "print(\n",
    "    \"train set size: %i samples, classes proportions: %.3f percent\"\n",
    "    % (len(train), 100 * (train.tensors[1] == 1).sum() / len(train))\n",
    ")\n",
    "print(\n",
    "    \"test set size: %i samples, classes proportions: %.3f percent\"\n",
    "    % (len(test), 100 * (test.tensors[1] == 1).sum() / len(test))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build lipschitz Model\n",
    "\n",
    "Here the experiments are done with a only fully-connected layers but `torchlip` also provides state-of-the-art 1-Lipschitz convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequential model contains a layer which is not a Lipschitz layer: Flatten(start_dim=1, end_dim=-1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): SpectralLinear(in_features=784, out_features=128, bias=True)\n",
       "  (2): FullSort()\n",
       "  (3): SpectralLinear(in_features=128, out_features=64, bias=True)\n",
       "  (4): FullSort()\n",
       "  (5): SpectralLinear(in_features=64, out_features=32, bias=True)\n",
       "  (6): FullSort()\n",
       "  (7): FrobeniusLinear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from deel import torchlip\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ninputs = 28 * 28\n",
    "wass = torchlip.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torchlip.SpectralLinear(ninputs, 128),\n",
    "    torchlip.FullSort(),\n",
    "    torchlip.SpectralLinear(128, 64),\n",
    "    torchlip.FullSort(),\n",
    "    torchlip.SpectralLinear(64, 32),\n",
    "    torchlip.FullSort(),\n",
    "    torchlip.FrobeniusLinear(32, 1),\n",
    ").to(device)\n",
    "\n",
    "wass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learn classification on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b252c73825743a8b3394408bdad3f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6da08d1eb0f47d1bf3139c765193fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=92.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6438a1fdde94e90a76dae48d72785f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=92.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f2381f2d284ae581533ff010be62a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=92.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2877b224def54faabfaf053e6ae31ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=92.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fea2ef66b724663aa4120687d2e17e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=92.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deel.torchlip.functional import kr_loss, hkr_loss, hinge_margin_loss\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# training parameters\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "# loss parameters\n",
    "min_margin = 1\n",
    "alpha = 10\n",
    "\n",
    "optimizer = torch.optim.Adam(lr=0.01, params=wass.parameters())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\n",
    "\n",
    "tepochs = trange(epochs)\n",
    "for _ in tepochs:\n",
    "    m_nc, m_kr, m_hm, m_acc = 0, 0, 0, 0\n",
    "\n",
    "    tdata = tqdm(train_loader)\n",
    "    wass.train()\n",
    "    for data, target in tdata:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = wass(data)\n",
    "        loss = hkr_loss(output, target, alpha=alpha, min_margin=min_margin)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        m_nc += 1\n",
    "        m_kr += kr_loss(output, target, (-1, 1))\n",
    "        m_hm += hinge_margin_loss(output, target, min_margin)\n",
    "        m_acc += (torch.sign(output).flatten() == torch.sign(target)).sum() / len(\n",
    "            target\n",
    "        )\n",
    "        tdata.set_postfix(\n",
    "            {\n",
    "                k: \"{:.04f}\".format(v)\n",
    "                for k, v in {\n",
    "                    \"loss\": loss,\n",
    "                    \"kr\": m_kr / m_nc,\n",
    "                    \"hm\": m_hm / m_nc,\n",
    "                    \"acc\": m_acc / m_nc,\n",
    "                }.items()\n",
    "            }\n",
    "        )\n",
    "\n",
    "    wass.eval()\n",
    "    testo = []\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        testo.append(wass(data).detach().cpu())\n",
    "    testo = torch.cat(testo).flatten()\n",
    "\n",
    "    postfix = {\n",
    "        f\"train_{k}\": \"{:.04f}\".format(v)\n",
    "        for k, v in {\n",
    "            \"loss\": loss,\n",
    "            \"kr\": m_kr / m_nc,\n",
    "            \"hm\": m_hm / m_nc,\n",
    "            \"acc\": m_acc / m_nc,\n",
    "        }.items()\n",
    "    }\n",
    "    postfix.update(\n",
    "        {\n",
    "            f\"val_{k}\": \"{:.04f}\".format(v)\n",
    "            for k, v in {\n",
    "                \"loss\": hkr_loss(\n",
    "                    testo, test.tensors[1], alpha=alpha, min_margin=min_margin\n",
    "                ),\n",
    "                \"kr\": kr_loss(testo.flatten(), test.tensors[1], (-1, 1)),\n",
    "                \"hm\": hinge_margin_loss(testo.flatten(), test.tensors[1], min_margin),\n",
    "                \"acc\": (torch.sign(testo).flatten() == torch.sign(test.tensors[1]))\n",
    "                .float()\n",
    "                .mean(),\n",
    "            }.items()\n",
    "        }\n",
    "    )\n",
    "\n",
    "    tepochs.set_postfix(postfix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Lipschitz constant of our networks\n",
    "\n",
    "### 4.1. Empirical evaluation\n",
    "\n",
    "We can estimate the Lipschitz constant by evaluating \n",
    "\n",
    "$$\n",
    "    \\frac{\\Vert{}F(x_2) - F(x_1)\\Vert{}}{\\Vert{}x_2 - x_1\\Vert{}} \\quad\\text{or}\\quad \n",
    "    \\frac{\\Vert{}F(x + \\epsilon) - F(x)\\Vert{}}{\\Vert{}\\epsilon\\Vert{}}\n",
    "$$\n",
    "\n",
    "for various inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1401)\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "wass.eval()\n",
    "\n",
    "p = []\n",
    "for _ in range(64):\n",
    "    eps = 1e-3\n",
    "    batch, _ = next(iter(train_loader))\n",
    "    dist = torch.distributions.Uniform(-eps, +eps).sample(batch.shape)\n",
    "    y1 = wass(batch.to(device)).detach().cpu()\n",
    "    y2 = wass((batch + dist).to(device)).detach().cpu()\n",
    "\n",
    "    p.append(\n",
    "        torch.max(\n",
    "            torch.norm(y2 - y1, dim=1)\n",
    "            / torch.norm(dist.reshape(dist.shape[0], -1), dim=1)\n",
    "        )\n",
    "    )\n",
    "print(torch.tensor(p).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14525beffff9416ba3c3849b8e1da694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=92.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor(0.9200, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "wass.eval()\n",
    "\n",
    "p = []\n",
    "for batch, _ in tqdm(train_loader):\n",
    "    x = batch.numpy()\n",
    "    y = wass(batch.to(device)).detach().cpu().numpy()\n",
    "    xd = pdist(x.reshape(batch.shape[0], -1))\n",
    "    yd = pdist(y.reshape(batch.shape[0], -1))\n",
    "\n",
    "    p.append((yd / xd).max())\n",
    "print(torch.tensor(p).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, using the $\\epsilon$-version, we greatly under-estimate the Lipschitz constant.\n",
    "Using the train dataset, we find a Lipschitz constant close to 0.9, which is better, but our network should be 1-Lipschitz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Singular-Value Decomposition\n",
    "\n",
    "Since our network is only made of linear layers and `FullSort` activation, we can compute *Singular-Value Decomposition* (SVD) of our weight matrix and check that, for each linear layer, all singular values are 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Before export ===\n",
      "SpectralLinear(in_features=784, out_features=128, bias=True), min=0.9999991059303284, max=1.0000009536743164\n",
      "SpectralLinear(in_features=128, out_features=64, bias=True), min=0.9999989867210388, max=1.0000009536743164\n",
      "SpectralLinear(in_features=64, out_features=32, bias=True), min=0.9999998211860657, max=1.0000001192092896\n",
      "FrobeniusLinear(in_features=32, out_features=1, bias=True), min=1.0, max=1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Before export ===\")\n",
    "layers = list(wass.children())\n",
    "for layer in layers:\n",
    "    if hasattr(layer, \"weight\"):\n",
    "        w = layer.weight\n",
    "        u, s, v = torch.svd(w)\n",
    "        print(f\"{layer}, min={s.min()}, max={s.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== After export ===\n",
      "Linear(in_features=784, out_features=128, bias=True), min=0.9999991059303284, max=1.0000009536743164\n",
      "Linear(in_features=128, out_features=64, bias=True), min=0.9999989867210388, max=1.0000009536743164\n",
      "Linear(in_features=64, out_features=32, bias=True), min=0.9999998211860657, max=1.0000001192092896\n",
      "Linear(in_features=32, out_features=1, bias=True), min=1.0, max=1.0\n"
     ]
    }
   ],
   "source": [
    "wexport = wass.vanilla_export()\n",
    "\n",
    "print(\"=== After export ===\")\n",
    "layers = list(wexport.children())\n",
    "for layer in layers:\n",
    "    if hasattr(layer, \"weight\"):\n",
    "        w = layer.weight\n",
    "        u, s, v = torch.svd(w)\n",
    "        print(f\"{layer}, min={s.min()}, max={s.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all our singular values are very close to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
